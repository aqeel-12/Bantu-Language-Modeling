{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CCX___4GVQYa"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aqeel-12/Bantu-Language-Modeling/blob/NLP-Challenge-4/NLP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFeg-Zqpe6AV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baaf4657-5f25-4277-99aa-d4eb152c8a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference link:\n",
        "https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0"
      ],
      "metadata": {
        "id": "q6WlPm4VmGFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n"
      ],
      "metadata": {
        "id": "0JDOhM3_fNDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(text):\n",
        "  # Finding ngrams\n",
        "  chars_tokens = (n)*['<START>'] + text  \n",
        "  ngrams_nextChars = [(tuple(chars_tokens[i:i+n]),chars_tokens[i+n]) for i in range(len(chars_tokens)-n)]\n",
        "  return ngrams_nextChars    \n",
        "\n",
        "def addVal_to_dictKey(D, key, val=1):\n",
        "  if key in D:\n",
        "    if type(val)==int:\n",
        "      D[key] += val\n",
        "    elif (type(val)==str) & (val!='<START>'):\n",
        "      D[key].append(val)\n",
        "  else:\n",
        "    if type(val)==int:\n",
        "      D[key] = val\n",
        "    elif (type(val)==str) & (val!='<START>'):\n",
        "      D[key] = [val]\n",
        "  return D\n",
        "\n",
        "def find_probs_of_dict(D):\n",
        "  total = 0\n",
        "  for key in D.keys():\n",
        "    total += D[key]\n",
        "  for key in D.keys():\n",
        "    D[key] /= total\n",
        "  return D \n",
        "\n",
        "def get_dict_from_list(L):\n",
        "  D = {}\n",
        "  for char in L:\n",
        "    if char in D:\n",
        "      D[char] += 1\n",
        "    else:\n",
        "      D[char] = 1\n",
        "  return D\n",
        "\n",
        "def max_of_dict(D):  \n",
        "  maxProb = 0\n",
        "  maxKey = ''\n",
        "  for key, probVal in D.items():\n",
        "    if probVal>maxProb:\n",
        "      maxProb = probVal\n",
        "      maxKey = key\n",
        "  return (maxKey, maxProb)\n",
        "\n"
      ],
      "metadata": {
        "id": "bhcVhhRA3biH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filePath):\n",
        "  data_cwe = open(filePath, 'r').read().lower()\n",
        "  return data_cwe\n",
        "\n",
        "def clean_data(dataset: str) -> list:\n",
        "  \"\"\"\n",
        "  Function task : It will extract each and every character form the dataset which is an ASCII character without losing the order of the characters\n",
        "  -> Input Parameters --------------------\n",
        "  --> dataset : String of paragraph in text\n",
        "  \n",
        "  -> Output Parameters --------------------\n",
        "  --> all_characters_in_data : List all the ASCII characters from the dataset without losing the order of characters\n",
        "  --> data_clean : Paragraph in text after cleaning.\n",
        "  \"\"\"\n",
        "  ASCII_Chars = ' !\"\\'(),-.0123456789:;?abcdefghijklmnopqrstuvwxyz'\n",
        "  all_characters_in_data = re.findall(r\"[%s]\"%ASCII_Chars, dataset)\n",
        "  # data_clean = \"\".join(all_characters_in_data)\n",
        "  return all_characters_in_data\n",
        "\n",
        "\n",
        "def char_probability(list_of_chars: list) -> dict:\n",
        "  \"\"\"\n",
        "  Funciton task: \n",
        "\n",
        "  Input Parameters : \n",
        "  -> list_of_chars: \n",
        "\n",
        "  Output Parameter:\n",
        "  -> char_prob : \n",
        "  \"\"\"\n",
        "  total_chat_count = len(list_of_chars)\n",
        "  ASCII_Chars = ' !\"\\'(),-.0123456789:;?abcdefghijklmnopqrstuvwxyz'\n",
        "  char_prob = {char:0 for char in ASCII_Chars}\n",
        "\n",
        "  # Finding a number for each character how many times it is repeated in the given dataset\n",
        "  for char in list_of_chars:\n",
        "    char_prob[char] += 1\n",
        "  \n",
        "  # Finding probabilities of each character\n",
        "  for char in char_prob.keys():\n",
        "    char_prob[char] /= total_chat_count\n",
        "  \n",
        "  return char_prob\n",
        "\n",
        "def get_cond_prob(next_chars_of_ngram_prob):\n",
        "  for ngram in next_chars_of_ngram_prob:\n",
        "    ngramProb = ngram_prob[ngram]\n",
        "    poss_next_chars = next_chars_of_ngram_prob[ngram]\n",
        "    for key in poss_next_chars:\n",
        "      poss_next_chars[key] = poss_next_chars[key] * char_prob[key] * ngramProb\n",
        "    next_chars_of_ngram_prob[ngram] = poss_next_chars\n",
        "  return next_chars_of_ngram_prob\n",
        "\n",
        "def fit(all_characters_in_data, n=15):\n",
        "\n",
        "  char_prob = char_probability(all_characters_in_data)\n",
        "\n",
        "  all_ngrams = []\n",
        "  ngram_prob = {}\n",
        "  next_chars_of_ngram_prob = {}\n",
        "\n",
        "  # chars_tokens = (n)*['<START>'] + all_characters_in_data\n",
        "\n",
        "  # ===================================================================\n",
        "  # Finding ngrams, ngrams-count & ngrams-next_chars\n",
        "  # Finding ngrams (15-grams) & next-chat\n",
        "  ngrams_nextChars = get_ngrams(all_characters_in_data)\n",
        "  \n",
        "  for ngram, next_char in ngrams_nextChars:\n",
        "    all_ngrams.append(ngram)\n",
        "    ngram_prob = addVal_to_dictKey(ngram_prob, ngram, 1) # (D, key, val)\n",
        "    next_chars_of_ngram_prob = addVal_to_dictKey(next_chars_of_ngram_prob, ngram, next_char) # (D, key, val)\n",
        "  all_ngrams = list(set(all_ngrams))\n",
        "\n",
        "  # ===================================================================\n",
        "  # Finding the probabilities of each ngram-count\n",
        "  ngram_prob = find_probs_of_dict(ngram_prob)\n",
        "  \n",
        "  # ===================================================================\n",
        "  # Finding the probabilities of each possible next_chars of each ngrams\n",
        "  for ngram in next_chars_of_ngram_prob:\n",
        "    possible_chars = next_chars_of_ngram_prob[ngram]  # it is a list\n",
        "\n",
        "    # Finding count of each character repeated in ngram\n",
        "\n",
        "    possible_chars_dict_prob = get_dict_from_list(next_chars_of_ngram_prob[ngram])\n",
        "    next_chars_of_ngram_prob[ngram] = find_probs_of_dict(possible_chars_dict_prob)\n",
        "    \n",
        "    next_chars_of_ngram_condProb = get_cond_prob(next_chars_of_ngram_prob)\n",
        "  return char_prob, ngram_prob, next_chars_of_ngram_prob, next_chars_of_ngram_condProb # all_ngrams, \n",
        "\n",
        "def generate_next_char_of_ngram(ngram):\n",
        "  ngram = tuple(ngram[-15:])\n",
        "  poss_next_chars = next_chars_of_ngram_condProb[ngram]\n",
        "  \n",
        "  print(\"possible next characters --------------------- \")\n",
        "  print(list(poss_next_chars.keys()))\n",
        "  print(\"\\n\")\n",
        "\n",
        "  result = max_of_dict(poss_next_chars)\n",
        "  print(f\"Next possible character : {result[0]} ({result[1]})\")\n",
        "  return result\n",
        "\n",
        "def evaluate(ngram, char):\n",
        "  ngram = tuple(ngram[-15:])\n",
        "  poss_next_chars = next_chars_of_ngram_condProb[ngram]\n",
        "  print(\"possible next characters --------------------- \")\n",
        "  print(list(poss_next_chars.keys()))\n",
        "  print(\"\\n\")\n",
        "  result = max_of_dict(poss_next_chars)\n",
        "  \n",
        "  print(f\"Next Character : {result[0]}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # finding loss\n",
        "  if char in poss_next_chars.keys():\n",
        "    char_prob = poss_next_chars[char]\n",
        "  else:\n",
        "    char_prob = 0\n",
        "  loss = abs(result[1]-char_prob)\n",
        "  print(\"Loss : \", loss)\n",
        "  return loss\n",
        "\n",
        "def cross_entropy_loss(input_ngram: str):\n",
        "  ngrams_nextChars = get_ngrams(list(input_ngram))\n",
        "  total_loss = 0\n",
        "  for ngram, char in ngrams_nextChars:\n",
        "    if ngram in next_chars_of_ngram_condProb:\n",
        "      poss_next_chars = next_chars_of_ngram_condProb[ngram]\n",
        "      if char in poss_next_chars:\n",
        "        total_loss += -math.log2(poss_next_chars[char])\n",
        "      else:\n",
        "        total_loss = 0\n",
        "      total_loss = total_loss/len(input_ngram)\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "gvXb0nSPVPiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training model with training dataset**"
      ],
      "metadata": {
        "id": "v8TVv8zlV1Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "startTime = datetime.now()\n",
        "\n",
        "filePath = \"/content/drive/MyDrive/NLP/Assignment03/data/cwe-train.txt\"\n",
        "dataset = load_data(filePath)\n",
        "\n",
        "all_characters_in_data = clean_data(dataset)\n",
        "char_prob, ngram_prob, next_chars_of_ngram_prob, next_chars_of_ngram_condProb = fit(all_characters_in_data, 15)\n",
        "\n",
        "endTime = datetime.now()\n",
        "print(\"Code Running Time : \", endTime-startTime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1eSXW3ebrNt",
        "outputId": "602b4b08-dcf1-47b9-b3dc-e144bc2bd454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code Running Time :  0:00:06.059846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating character**"
      ],
      "metadata": {
        "id": "Yx-p7HnsVwOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ngram = tuple(\"munhu yoyose yo\")\n",
        "_ = generate_next_char_of_ngram(input_ngram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odo10VdzLqjG",
        "outputId": "243cf95e-217e-422c-8541-1c7ddab6fb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "possible next characters --------------------- \n",
            "['h', 'm', 'y', 'n', 'l', 'g', 'j', 't', 'k', 'i', 'd', 'p', 's', 'v', 'z', 'b']\n",
            "\n",
            "\n",
            "Next possible character : n (2.7822353455746416e-06)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Model with actual next character**"
      ],
      "metadata": {
        "id": "SoehIVCGV6M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = evaluate(tuple(\"munhu yoyose yo\"), 'h')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBI5cm_-LqmR",
        "outputId": "a1812118-5e92-4875-86b1-803dd93d40c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "possible next characters --------------------- \n",
            "['h', 'm', 'y', 'n', 'l', 'g', 'j', 't', 'k', 'i', 'd', 'p', 's', 'v', 'z', 'b']\n",
            "\n",
            "\n",
            "Next possible character : n\n",
            "\n",
            "\n",
            "Loss :  2.4972467825798725e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Entropy**"
      ],
      "metadata": {
        "id": "UTQec9TCd6WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy_loss(\"u yoyose yohauhokela \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH0CSeNLLqpE",
        "outputId": "9cedfe63-6aae-4dd8-bfcd-c4d14e49656f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.101662996406211"
            ]
          },
          "metadata": {},
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test Dataset**"
      ],
      "metadata": {
        "id": "gGdml2LMeH39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = \"/content/drive/MyDrive/NLP/Assignment03/data/cwe-test.txt\"\n",
        "dataset_test = load_data(filePath)\n",
        "\n",
        "test_all_characters_in_data = clean_data(dataset_test)\n",
        "ngrams_nextChars = get_ngrams(test_all_characters_in_data)"
      ],
      "metadata": {
        "id": "o-s5c2T_cgYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TotalLoss = 0\n",
        "for ngram, char in ngrams_nextChars:\n",
        " TotalLoss =  cross_entropy_loss(\"\".join(ngram))\n",
        "print(TotalLoss) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQmKImKfa5FP",
        "outputId": "a327af6c-4ceb-4a3c-d772-818bfe7a417e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy_loss(\"munhu yoyose yohauhokela u\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIM2MMoefDGc",
        "outputId": "cc57345c-3eee-45b4-ee7e-b5d2360628d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9228443236821698"
            ]
          },
          "metadata": {},
          "execution_count": 408
        }
      ]
    }
  ]
}